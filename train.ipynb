{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickdraw Doodle\n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 96\n",
    "batch_size = 256\n",
    "model_path = 'model.json'\n",
    "weights_path = 'model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from lib.preprocessing import json_to_array, image_generator\n",
    "from lib.data import read_dataset_folder, get_classes, get_label_encoder_y, image_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set = 438123\n",
      "Test set = 34000\n",
      "CPU times: user 21 s, sys: 1.33 s, total: 22.3 s\n",
      "Wall time: 22.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset_train = read_dataset_folder('train_simplified/set_14/')\n",
    "dataset_test = read_dataset_folder('test_simplified/set_1/', frac=0.1)\n",
    "dataset_train['drawing'] = dataset_train['drawing'].apply(json_to_array)\n",
    "dataset_test['drawing'] = dataset_test['drawing'].apply(json_to_array)\n",
    "print('Training set =', len(dataset_train))\n",
    "print('Test set =', len(dataset_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = image_generator(dataset_train, include_y=True)\n",
    "test_generator = image_generator(dataset_test, include_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=[0], dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_y = dataset_train['word'].values\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "labelEncoder_y = get_label_encoder_y()\n",
    "label_y = labelEncoder_y.transform(label_y).reshape(-1, 1)\n",
    "oneHotEncoder = OneHotEncoder(categorical_features=[0])\n",
    "oneHotEncoder.fit(label_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = InceptionResNetV2(input_shape=(image_size, image_size, 1), weights=None, classes=len(get_classes()))\n",
    "classifier.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = open(model_path)\n",
    "model_json = model_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "classifier = model_from_json(model_json)\n",
    "classifier.load_weights(weights_path)\n",
    "classifier.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier.fit_generator(train_generator, steps_per_epoch=int(math.ceil(len(dataset_train)/batch_size)), epochs=1,\n",
    "                         validation_data=test_generator, validation_steps=int(math.ceil(len(dataset_test)/batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 30s, sys: 12 ms, total: 3min 30s\n",
      "Wall time: 3min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_json = classifier.to_json()\n",
    "with open(model_path, 'w') as model_file:\n",
    "    model_file.write(model_json)\n",
    "classifier.save_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
